---
title: "04_Modelling"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir= normalizePath('..'))
```

# Modelling

Analyses of the data that we have obtained and processed will be conducted in order to achieve the aim and answer the questions raised in the previous section of the report. The analysis would be organised according to the order of the questions listed in the report's Business Objective section.


## Which event types dominate task run times?

Application.events data will be used to determine which event is dominating the task run time. In the data preparation step, the time taken for each activity was calculated, and assigned to a variable called **application.events**. A box plot and density plot would be plotted to demonstrate the spread and mean of each event name duration to answer the question of which event dominates the event run time. In addition, these graphics will be grouped according to the level of visualization. Because the render time of each level can be different.


### Density Plot
```{r}
  ggplot(application.events, aes(x = elapsed_time, color = as.factor(level), fill = as.factor(level))) +
    geom_density(alpha=0.5) +
    ggtitle("Distribution of Events Density Plot")+
    facet_wrap( ~ eventName, scales = "free", ncol = 3)
```
### Box Plot

```{r}
  ggplot(application.events, aes(x = elapsed_time, color = as.factor(level), fill = as.factor(level))) +
    geom_boxplot(alpha=0.5) +
    ggtitle("Distribution of Events Density  Box Plot")+
    facet_wrap( ~ eventName, scales = "free", ncol = 3)
```
The **Total Render** event should be excluded as it represents the entire task run time. Therefore, it is evident from the graphs above that the **Render** event type dominates the event execution time. In general, regardless of the visualization level, when the running times of the activities are compared, the Render event covers **94.5%** of the total time while producing the terapixel image.


```{r}
application.events.without_tr  <- application.events %>% filter(eventName != "TotalRender")
entire_duration <- sum(application.events.without_tr$elapsed_time)
```

```{r}
application.events.without_tr %>% group_by(eventName) %>% summarise(percent = sum(elapsed_time)/entire_duration*100)
```


## What is the interplay between GPU temperature and performance?

We'll use **gpu** data and **application.tasks** to answer the question about the relationship between GPU temperature and GPU performance. Generally, the performance of a GPU is assessed by how well it renders an object. However, there is no information about the quality of the tile of image. Therefore, we'll assume every tile is equal and evaluate GPU performance in terms of render time.

We have data about the render time for each tile. For each of these tiles, we will get the temperature values from the start and end time intervals of the tiles in the GPU data. Thus, we will obtain temperature data series for each render time. We'll look at the relationship between the elapsed time and GPU temperature during this process to see if GPU temperature has an impact on GPU performance. To investigate the correlation, we will use temperature values such as **mean,median, standard deviation, maximum, and minimum, coefficient of variation, quantiles**.

First, the table that correlates the temperature data and the elapsed time of the tiles needs to be created.


```{r}
source("src/associate_elapsed_time_with_gpu_values.R")
```


```{r}
df_temp_cor <- cor(df_temp[, c("elapsed_time", "MEAN","MEDIAN","SD", "COV" , "MIN","MAX","Q1","Q3")])
df_temp_cor
```

According to the correlation table, there does not appear to be a strong correlation between task duration and temperature attributes,but the elapsed time has a positive correlation of **0.56** with the temperature standard deviation and coefficient of variation. It means that how much temperature values are spread out around the mean or average affect elapsed time.


```{r}
corrplot(df_temp_cor, type = "upper", tl.col = "black", tl.srt = 45)
```


On the other hand, the performance and capacities of the 1024 GPU cards in the system may differ. Collective evaluation can be misleading. Therefore, a comparison between temperature and  performance should be made per GPU.    


```{r}
df_temp_cor <- df_temp %>%
  group_by(machine_id) %>%
  summarize(cor_sd=cor(SD,elapsed_time),cor_cov=cor(COV,elapsed_time),cor_mean=cor(MEAN,elapsed_time), cor_median=cor(MEDIAN,elapsed_time),cor_min=cor(MIN,elapsed_time),cor_max=cor(MAX,elapsed_time), cor_q1=cor(Q1,elapsed_time), cor_q3= cor(Q3,elapsed_time))

```



```{r}
gather(df_temp_cor, x, y, cor_sd:cor_q3) %>%
  ggplot(aes(x = y, color=x)) +
    geom_histogram() +
    ggtitle("Histogram of Temperature-Render Time Corelation by machine")+
    facet_wrap( ~ x, scales = "free", ncol = 2)
```

According to the histogram above, the correlation coefficients between the elapsed time and the standard deviation of the temperature values spread approximately between 0 and 0.85. Peak coefficients range from about 0.6 to 0.75, and  the histogram is slightly skewed to the left. It can be said that there is a small positive correlation. This interpretation will also be same for the coefficient of variance. The 2 histograms are very similar to each other. Furthermore, there is some correlation between the maximum temperature and the elapsed time. Peak values appear above but,itis not a strong correlation. Looking at other metrics, it seems that the correlation coefficients are distributed over small values.


## What is the interplay between increased power draw and render time?

To carry out this analysis, the methods performed in the previous stage will be followed. The correlation between the data in the **Power Draw** column in the GPU data and the elapsed time will be examined. 


```{r}
df_power_cor <- cor(df_power[, c("elapsed_time", "MEAN","MEDIAN","SD", "COV", "MIN","MAX","Q1","Q3")])
df_power_cor
```

According to the correlation table, many temperature statistics seem to have a correlation of more than 0.5 with elapsed time. These temperature statistical values are as follows; **mean, standart deviation, max, third quantile*. However, correlations are not strong.


```{r}
corrplot(df_power_cor, type = "upper", tl.col = "black", tl.srt = 45)
```




```{r}
df_power_cor <- df_power %>%
  group_by(machine_id) %>%
  summarize(cor_sd=cor(SD,elapsed_time),cor_cov=cor(COV,elapsed_time),cor_mean=cor(MEAN,elapsed_time), cor_median=cor(MEDIAN,elapsed_time),cor_min=cor(MIN,elapsed_time),cor_max=cor(MAX,elapsed_time), cor_q1=cor(Q1,elapsed_time), cor_q3= cor(Q3,elapsed_time))
```



```{r}
gather(df_power_cor, x, y, cor_sd:cor_q3) %>%
  ggplot(aes(x = y, color=x)) +
    geom_histogram() +
    ggtitle("Power -Render Time Corelation by machine")+
    facet_wrap( ~ x, scales = "free", ncol = 2)
```

According to the histogram above, the correlation coefficients between the elapsed time and the standard deviation of the power draw peak values spread approximately between 0.60 and 0.725, and slightly skewed to the left. The correlation coefficients of the maximum and median values also seem to be quite high. It seems that there is a positive correlation between performance and performance in the third quarter.





## Can we quantify the variation in computation requirements for particular tiles?

Each tile has different rendering costs in the production of terapixel image. Because each tile contains different information from each other. These complexities affect the processing time and resources required to create the tile. To perform this analysis, we will show the * *average gpu usage, power usage, gpu memory usage and elapsed time** required for each pixel on a heatmap.


```{r}

 tiles_power <- ggplot(df_power, aes(x= y, y = 255- x, color=MEAN)) +  geom_tile()  + scale_colour_gradient(
  low = "#1d2671",high = "#dddddd") + ggtitle("Power usage on tiles") + theme(axis.title.x = element_blank(), axis.title.y = element_blank()) 

 tiles_gpu_util <- ggplot(df_gpu_util, aes(x= y, y = 255- x, color=MEAN)) +  geom_tile()  + scale_colour_gradient(
  low = "#1d2671",high = "#dddddd") + ggtitle("Gpu Utilization on tiles") + theme(axis.title.x = element_blank(), axis.title.y = element_blank()) 
 
  tiles_gpu_memory_util <- ggplot(df_gpu_memory_util, aes(x= y, y = 255- x, color=MEAN)) +  geom_tile()  + scale_colour_gradient(low = "#1d2671",high = "#dddddd") + ggtitle("Gpu Memory Utilization on tiles") + theme(axis.title.x = element_blank(), axis.title.y = element_blank()) 
  
  tiles_elapsed_time <- ggplot(df_temp, aes(x= y, y = 255- x, color=elapsed_time)) +  geom_tile()  + scale_colour_gradient(low = "#1d2671",high = "#dddddd") + ggtitle("Elapsed Time on tiles") + theme(axis.title.x = element_blank(), axis.title.y = element_blank()) 
  

```


The following heatmaps show the relative impact of rendering each pixel in terms of a given average GPU metrics, time and power. Dark colors indicate low values of the metric, and light colors indicate large values of the metric.

It is clear that the visual legend in the upper left corner of the terapixel image was rendered in a short time using less power. Likewise, the lake at Leazes Park and the structures on the university campus were built with low power and processed quickly. Moreover, buildings and roads are distinguished on the heatmap,because there is difference between the computations required to render streets and buildings. In addition, the initial values of the x-axis seem to be quite intense in the gpu heatmaps, but the elapsed time is quite high. At the beginning of the process, it can be concluded that the system does not work efficiently.


```{r}
ggarrange(tiles_power, tiles_gpu_util, tiles_gpu_memory_util, tiles_elapsed_time, 
          labels = c("1", "2", "3", "4"), 
          ncol = 2, nrow = 2)
```




## Can we identify particular GPU cards whose performance differs to other cards?

In the data understanding step, we inferred that there might be a pattern in the gpu serial numbers. 
Unfortunately, we do not have any information about the capabilities of GPU cards. According to the plot below, it is estimated that there are 12 GPU models.

```{r}
ggplot(df_power, aes(x=gpuSerial, y=elapsed_time)) +
    geom_point() + ggtitle("GPU Performances by Gpu Serial")+
  theme_bw()
```


We can divide these serial numbers into 12 according to their first 4 digits. 

```{r}
df_power$gpuID <- as.numeric(substr(df_power$gpuSerial, 1, 4)) 
unique(df_power$gpuID)
```



```{r}
df_elapsed_time_gpu <- df_power %>% group_by(gpuID) %>% summarize(avg_elapsed = mean(elapsed_time)) 

gpu_diff_1 <- ggplot(df_elapsed_time_gpu, aes(x=as.factor(gpuID), y=avg_elapsed,  fill=avg_elapsed)) +
    geom_bar(stat = "identity") + ggtitle("GPU Model") + scale_fill_gradient(name = "avg time") + 
  theme_bw() + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1)) 

gpu_diff_2 <-ggplot(df_power, aes(x=elapsed_time, group=gpuID, fill=as.factor(gpuID))) +
    geom_boxplot() + ggtitle("GPU Model")+  guides(fill=guide_legend(title="gpu id ")) + 
  theme_bw()
```


Box chart and bar graph were created to measure the performance of this 12 GPU card model. It was previously stated that the performance metric will be used as the render time. According to the charts below, the performances of the graphics cards are close to each other. However, it is clear that the performance of GPU cards starting with 3241 and 3243 is lower than the others.

```{r, warning=FALSE}
ggarrange(gpu_diff_1, gpu_diff_2,ncol = 2, nrow = 1)
```



## What can we learn about the efficiency of the task scheduling process?

To measure the efficiency of task scheduling, we need to calculate how much delay there is in scheduling for each machine. We will use the **application.tasks** data set to calculate this. We will create a new data frame, and calculate difference the start of the first render time and the end of the last one for each machine. It will be called the **expected time**. Then, we will sum the total duration of the tasks for each machine. The difference between these 2 calculations will  give the machine's total **delay time**.


```{r}
source("src/calculate_delay.R")
```


```{r}
summary(machine_schedule_time$delay_time)
```

```{r}
hist(machine_schedule_time$delay_time, main="Delay Time Histogram")
```

According to summary, the delay time of machines in the task scheduling process ranges from **125.5 seconds** to **178.8 seconds**. The average time lost is **151.1 seconds**. The 3 machines with the most delays are as follows.

```{r}
machine_schedule_time %>% arrange(desc(delay_time)) %>% head(3)
```



